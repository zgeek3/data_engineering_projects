{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "084a2a126a944761b3448fca2ecdc188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>8</td><td>application_1587236385986_0009</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-31-20.us-west-2.compute.internal:20888/proxy/application_1587236385986_0009/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-28-212.us-west-2.compute.internal:8042/node/containerlogs/container_1587236385986_0009_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to my EMR Notebook!"
     ]
    }
   ],
   "source": [
    "print(\"Welcome to my EMR Notebook!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "733bb8194e904814973ec3843fdadd7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setting up the imports that will be needed for the run\n",
    "\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format, dayofweek\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DecimalType, LongType, TimestampType\n",
    "from pyspark.sql.functions import monotonically_increasing_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35806cc7b8bd43cf9c2b844074d284f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_song_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    This function will: \n",
    "     * Load song data from the S3 input.\n",
    "     * Extract data for the songs table and write to parquet files on S3.\n",
    "     * Extract data for the artists table and write to parquet files on S3.\n",
    "    \n",
    "\n",
    "    Parameters:\n",
    "        spark: spark connection\n",
    "        input_data: S3 path to the song parquet files\n",
    "        output_data: S3 path where the parquet files will be saved\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # get filepath to song data file\n",
    "    song_data =input_data\n",
    "    \n",
    "    # read song data file\n",
    "    df = spark.read.json(song_data)  \n",
    "\n",
    "    # extract columns to create songs table\n",
    "    songs_table = df.select(\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\").dropDuplicates()\n",
    "    \n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    songs_table.write.parquet(path=os.path.join(output_data,'songs.parquet'), \\\n",
    "        mode='overwrite',partitionBy = ('year','artist_id'))\n",
    "\n",
    "    # extract columns to create artists table\n",
    "    artists_table = df.selectExpr(\"artist_id\", \"artist_name as name\", \\\n",
    "        \"artist_location as location\",\"artist_latitude as latitude\", \\\n",
    "        \"artist_longitude as longitude\").dropDuplicates() \n",
    "    \n",
    "    # write artists table to parquet files\n",
    "    artists_table.write.parquet(path=os.path.join(output_data, \\\n",
    "        'artists.parquet'),mode='overwrite')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397ec06806934e03a480d6798049cf26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_log_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    This function will: \n",
    "     * Load log data from the S3 input.\n",
    "     * Extract data for the users table and write to parquet files on S3.\n",
    "     * Extract data for the timestamp table and write to parquet files on S3.\n",
    "     * Combine tables to create a songplays table and write to parquet files on S3.\n",
    "    \n",
    "\n",
    "    Parameters:\n",
    "        spark: spark connection\n",
    "        input_data: S3 path to the log parquet files\n",
    "        output_data: S3 path where the parquet files will be saved\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # get filepath to log data file\n",
    "    log_data = input_data\n",
    "\n",
    "    # read log data file\n",
    "    df = spark\\\n",
    "        .read\\\n",
    "        .option('inferSchema', True)\\\n",
    "        .json(log_data)\\\n",
    "        .cache() \n",
    "    \n",
    "    # filter by actions for song plays\n",
    "    df = df.filter(df.page ==\"NextSong\")\n",
    "\n",
    "    # extract columns for users table    \n",
    "    users_table = df.selectExpr(\"userId as user_id\", \"firstName as first_name\", \"lastName as last_name\", \\\n",
    "                                \"gender\", \"level\").dropDuplicates() \n",
    "    \n",
    "    # write users table to parquet files\n",
    "    users_table.write.parquet(path=os.path.join(output_data,'users.parquet'),mode='overwrite')\n",
    "    \n",
    "    # create timestamp column from original timestamp column\n",
    "    get_timestamp = udf(lambda x: int(int(x)/1000), IntegerType())\n",
    "    df = df.withColumn('new_ts', get_timestamp(df.ts))\n",
    "    \n",
    "    # create datetime column from original timestamp column\n",
    "    get_datetime = udf(lambda x: datetime.fromtimestamp((x/1000.0)), TimestampType()) \n",
    "    df =  df.withColumn('timestamp', get_datetime(df.ts))\n",
    "    \n",
    "    df = df.withColumn('hour',hour(df.timestamp))\n",
    "    df = df.withColumn('day',dayofmonth(df.timestamp))\n",
    "    df = df.withColumn('week',weekofyear(df.timestamp))\n",
    "    df = df.withColumn('month',month(df.timestamp))\n",
    "    df = df.withColumn('year',year(df.timestamp))\n",
    "    df = df.withColumn('weekday',dayofweek(df.timestamp))\n",
    "    \n",
    "    # extract columns to create time table\n",
    "    time_table = df.selectExpr(\"ts as start_time\", \"hour\", \"day\", \\\n",
    "        \"week\", \"month\",\"year\",\"weekday\").dropDuplicates() \n",
    "    \n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    time_table.write.parquet(path=os.path.join(output_data,'time.parquet'),mode='overwrite', \\\n",
    "        partitionBy = ('year','month'))\n",
    "\n",
    "    # read in song data to use for songplays table\n",
    "    song_df = spark.read.parquet(os.path.join(output_data,'songs.parquet'))\n",
    "\n",
    "    # read in song data to use for artists table\n",
    "    artists_df = spark.read.parquet(os.path.join(output_data,'artists.parquet'))\n",
    "\n",
    "    # read in song data to use for artists table\n",
    "    time_df = spark.read.parquet(os.path.join(output_data,'time.parquet'))\n",
    "\n",
    "    # extract columns from joined song and log datasets to create songplays table \n",
    "\n",
    "    # Joining the artist_id (from the artists table) to the log file to create songplay dataframe\n",
    "    songplay_table = df.join(artists_df, [df.artist == artists_df.name]) \\\n",
    "        .select(df.ts, df.userId, df.song,artists_df.artist_id,df.level,df.sessionId,df.location, \\\n",
    "        df.userAgent,df.song)\n",
    "\n",
    "    # Joining the song_id (from the songs table) to the songplay dataframe\n",
    "    songplay_table = songplay_table.join(song_df,[songplay_table.artist_id == song_df.artist_id, \\\n",
    "        songplay_table.song == song_df.title]).select(songplay_table.ts, songplay_table.userId, \\\n",
    "        songplay_table.level,song_df.song_id,songplay_table.artist_id,songplay_table.sessionId, \\\n",
    "        songplay_table.location,songplay_table.userAgent)\n",
    "\n",
    "    # Joining the year and month (from the time table) to the songplay dataframe (used for partitioning)\n",
    "    songplay_table = songplay_table.join(time_df,[songplay_table.ts == time_df.start_time]). \\\n",
    "    select(songplay_table.ts, songplay_table.userId, songplay_table.level,songplay_table.song_id, \\\n",
    "        songplay_table.artist_id,songplay_table.sessionId,songplay_table.location, \\\n",
    "        songplay_table.userAgent,time_df.year,time_df.month)\n",
    "\n",
    "    # Renaming the songplay columns to the desired text\n",
    "    songplay_table = songplay_table.selectExpr(\"ts as start_time\",\"userId as user_id\",\"song_id\", \\\n",
    "        \"artist_id\",\"sessionId as session_id\",\"location\",\"userAgent as user_agent\",\"year\",\"month\")\n",
    " \n",
    "    # Creating the songplays songplay_id\n",
    "\n",
    "    songplay_table = songplay_table.select(\"*\").withColumn(\"songplay_id\", \\\n",
    "        monotonically_increasing_id()).selectExpr(\"songplay_id\",\"start_time\",\"user_id\" \\\n",
    "        ,\"song_id\",\"artist_id\",\"session_id\",\"location\",\"user_agent\",\"year\",\"month\")\n",
    "\n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    songplay_table.write.parquet(path=os.path.join(output_data,'songplays.parquet'), \\\n",
    "        mode='overwrite',partitionBy = (\"year\",\"month\"))\n",
    "\n",
    "    songplay_table.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a64b7c0c16342918e1d239ce55d5852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_song_data_start():\n",
    "    \"\"\"\n",
    "    This function will: \n",
    "        * Setup the input and outputs for prossessing the song data\n",
    "    Parameters:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    input_song_data = \"s3a://udacity-dend/song_data/*/*/*/*.json\"\n",
    "    output_data = \"s3://<>\" # replace with your own storage bucket\n",
    "    process_song_data(spark, input_song_data, output_data)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d2b197951941a08d6fb708487f74f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_log_data_start():\n",
    "    \"\"\"\n",
    "    This function will: \n",
    "        * Setup the input and outputs for prossessing the log data\n",
    "    Parameters:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    input_log_data = \"s3a://udacity-dend/log_data/*/*/*.json\"\n",
    "    output_data = \"s3a://<>\" # replace with your own storage bucket  \n",
    "    process_log_data(spark, input_log_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91bdc1a789af4c92a9f2b574327b266d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+------------------+------------------+----------+--------------------+--------------------+----+-----+\n",
      "|songplay_id|   start_time|user_id|           song_id|         artist_id|session_id|            location|          user_agent|year|month|\n",
      "+-----------+-------------+-------+------------------+------------------+----------+--------------------+--------------------+----+-----+\n",
      "|          0|1542788743796|     88|SOCHPTV12A6BD53113|ARN8NCB1187FB49652|       744|Sacramento--Rosev...|\"Mozilla/5.0 (Mac...|2018|   11|\n",
      "| 8589934592|1541535131796|     97|SODCQYZ12A6D4F9B26|ARYJ7KN1187B98CC73|       293|Lansing-East Lans...|\"Mozilla/5.0 (X11...|2018|   11|\n",
      "|17179869184|1541442367796|     73|SOKQFRT12A8C132F46|AR0N7RH1187B9B7497|       255|Tampa-St. Petersb...|\"Mozilla/5.0 (Mac...|2018|   11|\n",
      "+-----------+-------------+-------+------------------+------------------+----------+--------------------+--------------------+----+-----+\n",
      "only showing top 3 rows"
     ]
    }
   ],
   "source": [
    "# Running the pipeline\n",
    "\n",
    "process_song_data_start()\n",
    "process_log_data_start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
